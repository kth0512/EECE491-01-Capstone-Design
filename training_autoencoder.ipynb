{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"A100","authorship_tag":"ABX9TyM0x5SLFwl8+t6zYqQvOwP1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import sys\n","sys.path.append('/content/drive/MyDrive/commit_test_folder/EECE491-01-Capstone-Design')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T8QOVrMmu4kM","executionInfo":{"status":"ok","timestamp":1762061889650,"user_tz":-540,"elapsed":27208,"user":{"displayName":"종합설계","userId":"01573912834551956230"}},"outputId":"9f7699e4-f71e-40d9-f21a-742c3e2efc14"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UVMbH_klug9n","executionInfo":{"status":"ok","timestamp":1762061984444,"user_tz":-540,"elapsed":39625,"user":{"displayName":"종합설계","userId":"01573912834551956230"}},"outputId":"5d8161f6-d48a-4859-c815-6a2540985db1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Starting data setup...\n","Copying /content/drive/MyDrive/datasets/cropped_celeba.tar to local runtime...\n","Copy complete.\n","Untarring /content/cropped_celeba.tar to /content/celeba_dataset...\n","Untar complete.\n","Data setup finished in 39.60 seconds.\n","Successfully found data at: /content/celeba_dataset/content/cropped_celeba\n"]}],"source":["# This cell prepares the Colab environment by copying and\n","# extracting the dataset from Google Drive to the fast local SSD.\n","\n","import os\n","import time\n","\n","# --- Part 1: Colab Local Data Setup ---\n","print(\"Starting data setup...\")\n","start_setup_time = time.time()\n","\n","# --- Define paths ---\n","DRIVE_ARCHIVE_PATH = \"/content/drive/MyDrive/datasets/cropped_celeba.tar\"\n","LOCAL_ARCHIVE_PATH = \"/content/cropped_celeba.tar\"\n","EXTRACT_PATH = \"/content/celeba_dataset\"\n","\n","# The final, correct path to the images, based on our investigation\n","LOCAL_DATA_DIR = os.path.join(EXTRACT_PATH, \"content\", \"cropped_celeba\")\n","\n","# --- Logic ---\n","# Only copy/untar if the local data directory doesn't already exist\n","if not os.path.exists(LOCAL_DATA_DIR):\n","    print(f\"Copying {DRIVE_ARCHIVE_PATH} to local runtime...\")\n","    if not os.path.exists(DRIVE_ARCHIVE_PATH):\n","        print(f\"[FATAL ERROR] Source file not found: {DRIVE_ARCHIVE_PATH}\")\n","        raise FileNotFoundError(f\"Source file not found: {DRIVE_ARCHIVE_PATH}\")\n","\n","    # 1. Copy the single .tar file from Drive (fast)\n","    !cp \"{DRIVE_ARCHIVE_PATH}\" \"{LOCAL_ARCHIVE_PATH}\"\n","    print(\"Copy complete.\")\n","\n","    # 2. Extract the archive to the local SSD (fast)\n","    print(f\"Untarring {LOCAL_ARCHIVE_PATH} to {EXTRACT_PATH}...\")\n","    !mkdir -p \"{EXTRACT_PATH}\"\n","    !tar -xf \"{LOCAL_ARCHIVE_PATH}\" -C \"{EXTRACT_PATH}\"\n","    print(\"Untar complete.\")\n","\n","    # 3. Clean up the local archive to save space\n","    !rm \"{LOCAL_ARCHIVE_PATH}\"\n","else:\n","    print(f\"Data directory {LOCAL_DATA_DIR} already exists. Skipping copy/untar.\")\n","\n","print(f\"Data setup finished in {time.time() - start_setup_time:.2f} seconds.\")\n","\n","# --- Sanity Check ---\n","# Crucial check to ensure data exists before proceeding\n","if not os.path.exists(LOCAL_DATA_DIR):\n","    print(f\"\\n[FATAL ERROR] The expected data directory does not exist: {LOCAL_DATA_DIR}\")\n","    raise FileNotFoundError(f\"Could not find data at {LOCAL_DATA_DIR}\")\n","else:\n","    print(f\"Successfully found data at: {LOCAL_DATA_DIR}\")"]},{"cell_type":"code","source":["import torch\n","# Import the function from the data_utils.py file we created\n","from data_utils import get_dataloaders\n","\n","# --- Configuration ---\n","# Use the local data path defined in Cell 1\n","DATA_ROOT = LOCAL_DATA_DIR\n","BATCH_SIZE = 256\n","IMAGE_SIZE = 128\n","RANDOM_SEED = 42\n","\n","# --- 1. Get Dataloaders ---\n","# This single function call does all the work\n","train_loader, val_loader, test_loader = get_dataloaders(\n","    root_dir=DATA_ROOT,\n","    batch_size=BATCH_SIZE,\n","    image_size=IMAGE_SIZE,\n","    random_seed=RANDOM_SEED\n",")\n","\n","# --- 2. Verification ---\n","# Final check to ensure the dataloader works\n","if train_loader:\n","    print(\"\\nVerifying one batch from train_loader...\")\n","    try:\n","        # Get one sample batch\n","        images, labels = next(iter(train_loader))\n","        print(f\"  Batch loaded successfully.\")\n","        print(f\"  Image batch shape: {images.shape}\")\n","        print(\"\\nSetup complete. You are ready to start training.\")\n","    except Exception as e:\n","        print(f\"  [Error] Failed to load batch: {e}\")\n","else:\n","    print(\"\\nData loading failed. Please check previous cell output.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"USLmmEg_uihK","executionInfo":{"status":"ok","timestamp":1762061998105,"user_tz":-540,"elapsed":12292,"user":{"displayName":"종합설계","userId":"01573912834551956230"}},"outputId":"dec60814-3ee5-46b5-a2b0-fdb70cd3e82a"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Loading dataset from: /content/celeba_dataset/content/cropped_celeba\n","Searching for '*.jpg' files in: /content/celeba_dataset/content/cropped_celeba\n","Successfully found 199509 images.\n","Successfully loaded 199509 total images.\n","Splitting dataset into:\n","  Train: 159607 images\n","  Validation: 19950 images\n","  Test: 19952 images\n","\n","DataLoaders created successfully.\n","\n","Verifying one batch from train_loader...\n","  Batch loaded successfully.\n","  Image batch shape: torch.Size([256, 3, 128, 128])\n","\n","Setup complete. You are ready to start training.\n"]}]},{"cell_type":"code","source":["# (This is the main training/saving cell in training.ipynb)\n","# (This version includes the Validation loop and Best Model Saving)\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import random  # (For random SNR)\n","import os\n","\n","# --- 1. Import Modules ---\n","from channels import awgn_channel\n","from face_autoencoder import FaceAutoencoder\n","\n","# (We assume train_loader and val_loader are loaded from the previous cell)\n","\n","# -----------------------------------------------\n","# 2. Training Setup\n","# -----------------------------------------------\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","model = FaceAutoencoder(latent_dim=1024).to(device)\n","criterion = nn.MSELoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n","\n","# (Training parameters)\n","num_epochs = 20\n","MIN_SNR_DB = 0.0\n","MAX_SNR_DB = 20.0\n","\n","# (Save paths)\n","SAVE_DIR = \"/content/drive/MyDrive/models\"\n","# (We save the \"best\" model based on validation loss)\n","MODEL_PATH = os.path.join(SAVE_DIR, \"face_autoencoder_1024.pth\")\n","os.makedirs(SAVE_DIR, exist_ok=True)\n","\n","# Add a variable to track the best validation loss\n","best_val_loss = float('inf') # (Initialize with infinity)\n","\n","# -----------------------------------------------\n","# 3. Training Loop (with Validation)\n","# -----------------------------------------------\n","print(\"Starting robust training (Random SNR)...\")\n","# Define fixed SNR points for robustness evaluation (5 points)\n","SNR_POINTS_FOR_VAL = [0.0, 5.0, 10.0, 15.0, 20.0]\n","NUM_VAL_POINTS = len(SNR_POINTS_FOR_VAL)\n","\n","for epoch in range(num_epochs):\n","\n","    # --- (A) Training Phase ---\n","    model.train() # Set model to TRAINING mode\n","    total_train_loss = 0\n","\n","    for images, _ in train_loader:\n","        images = images.to(device)\n","\n","        # 1. Encode\n","        latent_vector = model.encode(images)\n","\n","        # 2. Channel (Apply random SNR for robust training)\n","        current_snr_db = random.uniform(MIN_SNR_DB, MAX_SNR_DB)\n","        noisy_vector = awgn_channel(latent_vector, snr_db=current_snr_db)\n","\n","        # 3. Decode\n","        reconstructed_images = model.decode(noisy_vector)\n","\n","        # 4. Loss\n","        loss = criterion(reconstructed_images, images)\n","\n","        # 5. Backward\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_train_loss += loss.item()\n","\n","    avg_train_loss = total_train_loss / len(train_loader)\n","\n","    # --- (B) Validation Phase (Robustness Evaluation) ---\n","    model.eval() # Set model to EVALUATION mode (Important)\n","    total_combined_loss = 0\n","\n","    # No gradients needed for validation (Saves memory/computation)\n","    with torch.no_grad():\n","        for val_images, _ in val_loader:\n","            val_images = val_images.to(device)\n","\n","            # Loop through fixed SNR points to calculate average loss\n","            for fixed_snr_db in SNR_POINTS_FOR_VAL:\n","\n","                # 1. Encode\n","                latent_vector = model.encode(val_images)\n","\n","                # 2. Channel (Apply fixed SNR for consistent evaluation)\n","                noisy_vector = awgn_channel(latent_vector, snr_db=fixed_snr_db)\n","\n","                # 3. Decode\n","                reconstructed_images = model.decode(noisy_vector)\n","\n","                # 4. Loss\n","                val_loss = criterion(reconstructed_images, val_images)\n","                total_combined_loss += val_loss.item()\n","\n","    # Calculate average validation loss across all batches and all SNR points\n","    avg_val_loss = total_combined_loss / (len(val_loader) * NUM_VAL_POINTS)\n","\n","    # --- (C) Log and Save Best Model ---\n","    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.6f}, Val Loss: {avg_val_loss:.6f}\")\n","\n","    # Save only if this epoch's Val Loss is the best one seen so far\n","    if avg_val_loss < best_val_loss:\n","        best_val_loss = avg_val_loss\n","        print(f\"  -> New best validation loss! Saving model to {MODEL_PATH}\")\n","        # Save the state_dict of the single model\n","        torch.save(model.state_dict(), MODEL_PATH)\n","\n","print(\"--- Training finished. ---\")\n","print(f\"Best validation loss achieved: {best_val_loss:.6f}\")\n","print(f\"Best model saved to {MODEL_PATH}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9GzLK_7wvmuq","executionInfo":{"status":"ok","timestamp":1762067076829,"user_tz":-540,"elapsed":2209904,"user":{"displayName":"종합설계","userId":"01573912834551956230"}},"outputId":"10570aae-6222-4b38-8584-5107d4ab783b"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","Starting robust training (Random SNR)...\n","Epoch [1/20], Train Loss: 0.091822, Val Loss: 0.044328\n","  -> New best validation loss! Saving model to /content/drive/MyDrive/models/face_autoencoder_1024.pth\n","Epoch [2/20], Train Loss: 0.037098, Val Loss: 0.030692\n","  -> New best validation loss! Saving model to /content/drive/MyDrive/models/face_autoencoder_1024.pth\n","Epoch [3/20], Train Loss: 0.029921, Val Loss: 0.026886\n","  -> New best validation loss! Saving model to /content/drive/MyDrive/models/face_autoencoder_1024.pth\n","Epoch [4/20], Train Loss: 0.024790, Val Loss: 0.023132\n","  -> New best validation loss! Saving model to /content/drive/MyDrive/models/face_autoencoder_1024.pth\n","Epoch [5/20], Train Loss: 0.021991, Val Loss: 0.021725\n","  -> New best validation loss! Saving model to /content/drive/MyDrive/models/face_autoencoder_1024.pth\n","Epoch [6/20], Train Loss: 0.020317, Val Loss: 0.019258\n","  -> New best validation loss! Saving model to /content/drive/MyDrive/models/face_autoencoder_1024.pth\n","Epoch [7/20], Train Loss: 0.018582, Val Loss: 0.017842\n","  -> New best validation loss! Saving model to /content/drive/MyDrive/models/face_autoencoder_1024.pth\n","Epoch [8/20], Train Loss: 0.017501, Val Loss: 0.027596\n","Epoch [9/20], Train Loss: 0.017006, Val Loss: 0.016360\n","  -> New best validation loss! Saving model to /content/drive/MyDrive/models/face_autoencoder_1024.pth\n","Epoch [10/20], Train Loss: 0.016266, Val Loss: 0.019820\n","Epoch [11/20], Train Loss: 0.015659, Val Loss: 0.015227\n","  -> New best validation loss! Saving model to /content/drive/MyDrive/models/face_autoencoder_1024.pth\n","Epoch [12/20], Train Loss: 0.014671, Val Loss: 0.014892\n","  -> New best validation loss! Saving model to /content/drive/MyDrive/models/face_autoencoder_1024.pth\n","Epoch [13/20], Train Loss: 0.013883, Val Loss: 0.014248\n","  -> New best validation loss! Saving model to /content/drive/MyDrive/models/face_autoencoder_1024.pth\n","Epoch [14/20], Train Loss: 0.014624, Val Loss: 0.017021\n","Epoch [15/20], Train Loss: 0.013916, Val Loss: 0.014110\n","  -> New best validation loss! Saving model to /content/drive/MyDrive/models/face_autoencoder_1024.pth\n","Epoch [16/20], Train Loss: 0.013170, Val Loss: 0.013444\n","  -> New best validation loss! Saving model to /content/drive/MyDrive/models/face_autoencoder_1024.pth\n","Epoch [17/20], Train Loss: 0.012736, Val Loss: 0.013631\n","Epoch [18/20], Train Loss: 0.012472, Val Loss: 0.012962\n","  -> New best validation loss! Saving model to /content/drive/MyDrive/models/face_autoencoder_1024.pth\n","Epoch [19/20], Train Loss: 0.014962, Val Loss: 0.013447\n","Epoch [20/20], Train Loss: 0.012410, Val Loss: 0.012561\n","  -> New best validation loss! Saving model to /content/drive/MyDrive/models/face_autoencoder_1024.pth\n","--- Training finished. ---\n","Best validation loss achieved: 0.012561\n","Best model saved to /content/drive/MyDrive/models/face_autoencoder_1024.pth\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import numpy as np\n","import os\n","import random\n","from PIL import Image\n","\n","# --- 1. Module and Dataloader Setup (Assuming these are defined earlier) ---\n","from channels import awgn_channel\n","from face_autoencoder import FaceAutoencoder\n","\n","# (Assume val_loader, device are loaded from the previous cell)\n","\n","# -----------------------------------------------\n","# 2. Configuration for Visualization\n","# -----------------------------------------------\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","SAVE_DIR = \"/content/drive/MyDrive/models\"\n","VISUALIZE_SNR_DB = 5.0\n","NUM_IMAGES_TO_SHOW = 4\n","BASE_OUTPUT_FOLDER = \"./reconstructions_comparison\"\n","os.makedirs(BASE_OUTPUT_FOLDER, exist_ok=True)\n","\n","# 모델 구성 목록: (latent_dim, model_file_suffix)\n","# 1024 차원 모델을 추가했습니다.\n","MODEL_CONFIGS = [\n","    (256, \"_256.pth\"),\n","    (512, \"_512.pth\"),\n","    (1024, \"_1024.pth\")\n","]\n","\n","# -----------------------------------------------\n","# 3. Utility Function: Image Saving (재사용)\n","# -----------------------------------------------\n","def save_image_to_pil(tensor, filename, output_folder):\n","    # 텐서를 NumPy 배열로 변환\n","    img_np = tensor.cpu().numpy()\n","    img_np = np.transpose(img_np, (1, 2, 0))\n","\n","    # Tanh 출력 [-1, 1] -> 픽셀 값 [0, 255]로 변환 및 uint8 타입으로 변경\n","    img_np = (img_np * 127.5) + 127.5\n","    img_np = np.clip(img_np, 0, 255).astype(np.uint8)\n","\n","    img_pil = Image.fromarray(img_np)\n","    img_pil.save(os.path.join(output_folder, filename))\n","\n","# -----------------------------------------------\n","# 4. Get a Batch of Validation Images (공통 사용)\n","# -----------------------------------------------\n","try:\n","    data_iter = iter(val_loader)\n","    images, _ = next(data_iter)\n","    images = images.to(device)\n","    sample_images = images[:NUM_IMAGES_TO_SHOW]\n","    print(f\"Successfully loaded {NUM_IMAGES_TO_SHOW} sample images for visualization.\")\n","except NameError:\n","    print(\"[ERROR] val_loader is not defined. Please run the Dataloader setup cell first. Exiting.\")\n","    exit()\n","except StopIteration:\n","    print(\"[ERROR] val_loader is empty or finished. Try rerunning the Dataloader cell. Exiting.\")\n","    exit()\n","\n","# --- 원본 이미지를 먼저 저장합니다 (세 모델이 공통으로 사용) ---\n","ORIGINAL_OUTPUT_FOLDER = os.path.join(BASE_OUTPUT_FOLDER, \"Original\")\n","os.makedirs(ORIGINAL_OUTPUT_FOLDER, exist_ok=True)\n","for i in range(NUM_IMAGES_TO_SHOW):\n","    save_image_to_pil(sample_images[i], f\"{i:02d}_Original.png\", ORIGINAL_OUTPUT_FOLDER)\n","print(f\"Original images saved to {ORIGINAL_OUTPUT_FOLDER}\")\n","\n","# -----------------------------------------------\n","# 5. Main Loop: Load, Process, and Save for Each Model\n","# -----------------------------------------------\n","for latent_dim, suffix in MODEL_CONFIGS:\n","    model_name = f\"face_autoencoder{suffix}\" # face_autoencoder_256.pth, face_autoencoder_512.pth 등\n","    MODEL_PATH = os.path.join(SAVE_DIR, model_name)\n","    OUTPUT_FOLDER = os.path.join(BASE_OUTPUT_FOLDER, f\"Latent_{latent_dim}\")\n","    os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n","\n","    print(f\"\\n--- Processing Model: Latent Dim = {latent_dim} ---\")\n","\n","    # 5.1 Load Model\n","    # 모델 호출 시 해당 latent_dim을 명시적으로 전달합니다.\n","    model = FaceAutoencoder(latent_dim=latent_dim).to(device)\n","    try:\n","        model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n","        model.eval()\n","        print(\"Model loaded successfully.\")\n","    except FileNotFoundError:\n","        print(f\"[ERROR] Model file not found at {MODEL_PATH}. Skipping this model.\")\n","        continue # 다음 모델로 넘어갑니다.\n","\n","    # 5.2 Process Images\n","    with torch.no_grad():\n","        latent_vector_original = model.encode(sample_images)\n","        # 노이즈를 추가합니다.\n","        noisy_latent_vector = awgn_channel(latent_vector_original, snr_db=VISUALIZE_SNR_DB)\n","\n","        # 1. 노이즈가 있는 잠재 벡터로 복원 (강건성 테스트)\n","        reconstructed_noisy = model.decode(noisy_latent_vector)\n","        # 2. 노이즈가 없는 깨끗한 잠재 벡터로 복원 (순수 복원 품질 테스트)\n","        reconstructed_pristine = model.decode(latent_vector_original)\n","\n","    # 5.3 Save Results\n","    image_list = [reconstructed_noisy, reconstructed_pristine]\n","    prefix_list = [f\"Recon_Noisy_{VISUALIZE_SNR_DB}dB\", \"Recon_Pristine\"]\n","\n","    for i in range(NUM_IMAGES_TO_SHOW):\n","        for j, (image_set, prefix) in enumerate(zip(image_list, prefix_list)):\n","            save_image_to_pil(image_set[i], f\"{i:02d}_{prefix}.png\", OUTPUT_FOLDER)\n","\n","    print(f\"Reconstructed images saved to {OUTPUT_FOLDER}\")\n","\n","print(\"\\n--- Comparison visualization completed. ---\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AfyzQfwLOcOF","executionInfo":{"status":"ok","timestamp":1762067906464,"user_tz":-540,"elapsed":1860,"user":{"displayName":"종합설계","userId":"01573912834551956230"}},"outputId":"a629ab57-351e-4d4f-dbdd-78adca93cda6"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Successfully loaded 4 sample images for visualization.\n","Original images saved to ./reconstructions_comparison/Original\n","\n","--- Processing Model: Latent Dim = 256 ---\n","Model loaded successfully.\n","Reconstructed images saved to ./reconstructions_comparison/Latent_256\n","\n","--- Processing Model: Latent Dim = 512 ---\n","Model loaded successfully.\n","Reconstructed images saved to ./reconstructions_comparison/Latent_512\n","\n","--- Processing Model: Latent Dim = 1024 ---\n","Model loaded successfully.\n","Reconstructed images saved to ./reconstructions_comparison/Latent_1024\n","\n","--- Comparison visualization completed. ---\n"]}]}]}