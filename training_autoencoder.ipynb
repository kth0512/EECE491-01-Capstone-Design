{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"A100","authorship_tag":"ABX9TyO+enLBW0+QQzp0CLRpIohT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import sys\n","sys.path.append('/content/drive/MyDrive/commit_test_folder/EECE491-01-Capstone-Design')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T8QOVrMmu4kM","executionInfo":{"status":"ok","timestamp":1761994617890,"user_tz":-540,"elapsed":2120,"user":{"displayName":"종합설계","userId":"01573912834551956230"}},"outputId":"a2555e47-7680-412f-e076-ca27c9a61d57"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UVMbH_klug9n","executionInfo":{"status":"ok","timestamp":1761994627872,"user_tz":-540,"elapsed":42,"user":{"displayName":"종합설계","userId":"01573912834551956230"}},"outputId":"3c7203d4-f64c-41e8-cc39-a35c946c7b56"},"outputs":[{"output_type":"stream","name":"stdout","text":["Starting data setup...\n","Data directory /content/celeba_dataset/content/cropped_celeba already exists. Skipping copy/untar.\n","Data setup finished in 0.00 seconds.\n","Successfully found data at: /content/celeba_dataset/content/cropped_celeba\n"]}],"source":["# This cell prepares the Colab environment by copying and\n","# extracting the dataset from Google Drive to the fast local SSD.\n","\n","import os\n","import time\n","\n","# --- Part 1: Colab Local Data Setup ---\n","print(\"Starting data setup...\")\n","start_setup_time = time.time()\n","\n","# --- Define paths ---\n","DRIVE_ARCHIVE_PATH = \"/content/drive/MyDrive/datasets/cropped_celeba.tar\"\n","LOCAL_ARCHIVE_PATH = \"/content/cropped_celeba.tar\"\n","EXTRACT_PATH = \"/content/celeba_dataset\"\n","\n","# The final, correct path to the images, based on our investigation\n","LOCAL_DATA_DIR = os.path.join(EXTRACT_PATH, \"content\", \"cropped_celeba\")\n","\n","# --- Logic ---\n","# Only copy/untar if the local data directory doesn't already exist\n","if not os.path.exists(LOCAL_DATA_DIR):\n","    print(f\"Copying {DRIVE_ARCHIVE_PATH} to local runtime...\")\n","    if not os.path.exists(DRIVE_ARCHIVE_PATH):\n","        print(f\"[FATAL ERROR] Source file not found: {DRIVE_ARCHIVE_PATH}\")\n","        raise FileNotFoundError(f\"Source file not found: {DRIVE_ARCHIVE_PATH}\")\n","\n","    # 1. Copy the single .tar file from Drive (fast)\n","    !cp \"{DRIVE_ARCHIVE_PATH}\" \"{LOCAL_ARCHIVE_PATH}\"\n","    print(\"Copy complete.\")\n","\n","    # 2. Extract the archive to the local SSD (fast)\n","    print(f\"Untarring {LOCAL_ARCHIVE_PATH} to {EXTRACT_PATH}...\")\n","    !mkdir -p \"{EXTRACT_PATH}\"\n","    !tar -xf \"{LOCAL_ARCHIVE_PATH}\" -C \"{EXTRACT_PATH}\"\n","    print(\"Untar complete.\")\n","\n","    # 3. Clean up the local archive to save space\n","    !rm \"{LOCAL_ARCHIVE_PATH}\"\n","else:\n","    print(f\"Data directory {LOCAL_DATA_DIR} already exists. Skipping copy/untar.\")\n","\n","print(f\"Data setup finished in {time.time() - start_setup_time:.2f} seconds.\")\n","\n","# --- Sanity Check ---\n","# Crucial check to ensure data exists before proceeding\n","if not os.path.exists(LOCAL_DATA_DIR):\n","    print(f\"\\n[FATAL ERROR] The expected data directory does not exist: {LOCAL_DATA_DIR}\")\n","    raise FileNotFoundError(f\"Could not find data at {LOCAL_DATA_DIR}\")\n","else:\n","    print(f\"Successfully found data at: {LOCAL_DATA_DIR}\")"]},{"cell_type":"code","source":["import torch\n","# Import the function from the data_utils.py file we created\n","from data_utils import get_dataloaders\n","\n","# --- Configuration ---\n","# Use the local data path defined in Cell 1\n","DATA_ROOT = LOCAL_DATA_DIR\n","BATCH_SIZE = 256\n","IMAGE_SIZE = 128\n","RANDOM_SEED = 42\n","\n","# --- 1. Get Dataloaders ---\n","# This single function call does all the work\n","train_loader, val_loader, test_loader = get_dataloaders(\n","    root_dir=DATA_ROOT,\n","    batch_size=BATCH_SIZE,\n","    image_size=IMAGE_SIZE,\n","    random_seed=RANDOM_SEED\n",")\n","\n","# --- 2. Verification ---\n","# Final check to ensure the dataloader works\n","if train_loader:\n","    print(\"\\nVerifying one batch from train_loader...\")\n","    try:\n","        # Get one sample batch\n","        images, labels = next(iter(train_loader))\n","        print(f\"  Batch loaded successfully.\")\n","        print(f\"  Image batch shape: {images.shape}\")\n","        print(\"\\nSetup complete. You are ready to start training.\")\n","    except Exception as e:\n","        print(f\"  [Error] Failed to load batch: {e}\")\n","else:\n","    print(\"\\nData loading failed. Please check previous cell output.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"USLmmEg_uihK","executionInfo":{"status":"ok","timestamp":1761994958568,"user_tz":-540,"elapsed":3928,"user":{"displayName":"종합설계","userId":"01573912834551956230"}},"outputId":"2ec5f8f3-2239-4114-82fd-b21ceceb6f64"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Loading dataset from: /content/celeba_dataset/content/cropped_celeba\n","Searching for '*.jpg' files in: /content/celeba_dataset/content/cropped_celeba\n","Successfully found 199509 images.\n","Successfully loaded 199509 total images.\n","Splitting dataset into:\n","  Train: 159607 images\n","  Validation: 19950 images\n","  Test: 19952 images\n","\n","DataLoaders created successfully.\n","\n","Verifying one batch from train_loader...\n","  Batch loaded successfully.\n","  Image batch shape: torch.Size([256, 3, 128, 128])\n","\n","Setup complete. You are ready to start training.\n"]}]},{"cell_type":"code","source":["# (This is the main training/saving cell in training.ipynb)\n","# (This version includes the Validation loop and Best Model Saving)\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import random  # (For random SNR)\n","import os\n","\n","# --- 1. Import Modules ---\n","from channels import awgn_channel\n","from face_autoencoder import FaceAutoencoder\n","\n","# (We assume train_loader and val_loader are loaded from the previous cell)\n","\n","# -----------------------------------------------\n","# 2. Training Setup\n","# -----------------------------------------------\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","model = FaceAutoencoder(latent_dim=256).to(device)\n","criterion = nn.MSELoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n","\n","# (Training parameters)\n","num_epochs = 20\n","MIN_SNR_DB = 0.0\n","MAX_SNR_DB = 20.0\n","\n","# (Save paths)\n","SAVE_DIR = \"/content/drive/MyDrive/models\"\n","# (We save the \"best\" model based on validation loss)\n","MODEL_PATH = os.path.join(SAVE_DIR, \"face_autoencoder_BEST.pth\")\n","os.makedirs(SAVE_DIR, exist_ok=True)\n","\n","# Add a variable to track the best validation loss\n","best_val_loss = float('inf') # (Initialize with infinity)\n","\n","# -----------------------------------------------\n","# 3. Training Loop (with Validation)\n","# -----------------------------------------------\n","print(\"Starting robust training (Random SNR)...\")\n","# Define fixed SNR points for robustness evaluation (5 points)\n","SNR_POINTS_FOR_VAL = [0.0, 5.0, 10.0, 15.0, 20.0]\n","NUM_VAL_POINTS = len(SNR_POINTS_FOR_VAL)\n","\n","for epoch in range(num_epochs):\n","\n","    # --- (A) Training Phase ---\n","    model.train() # Set model to TRAINING mode\n","    total_train_loss = 0\n","\n","    for images, _ in train_loader:\n","        images = images.to(device)\n","\n","        # 1. Encode\n","        latent_vector = model.encode(images)\n","\n","        # 2. Channel (Apply random SNR for robust training)\n","        current_snr_db = random.uniform(MIN_SNR_DB, MAX_SNR_DB)\n","        noisy_vector = awgn_channel(latent_vector, snr_db=current_snr_db)\n","\n","        # 3. Decode\n","        reconstructed_images = model.decode(noisy_vector)\n","\n","        # 4. Loss\n","        loss = criterion(reconstructed_images, images)\n","\n","        # 5. Backward\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_train_loss += loss.item()\n","\n","    avg_train_loss = total_train_loss / len(train_loader)\n","\n","    # --- (B) Validation Phase (Robustness Evaluation) ---\n","    model.eval() # Set model to EVALUATION mode (Important)\n","    total_combined_loss = 0\n","\n","    # No gradients needed for validation (Saves memory/computation)\n","    with torch.no_grad():\n","        for val_images, _ in val_loader:\n","            val_images = val_images.to(device)\n","\n","            # Loop through fixed SNR points to calculate average loss\n","            for fixed_snr_db in SNR_POINTS_FOR_VAL:\n","\n","                # 1. Encode\n","                latent_vector = model.encode(val_images)\n","\n","                # 2. Channel (Apply fixed SNR for consistent evaluation)\n","                noisy_vector = awgn_channel(latent_vector, snr_db=fixed_snr_db)\n","\n","                # 3. Decode\n","                reconstructed_images = model.decode(noisy_vector)\n","\n","                # 4. Loss\n","                val_loss = criterion(reconstructed_images, val_images)\n","                total_combined_loss += val_loss.item()\n","\n","    # Calculate average validation loss across all batches and all SNR points\n","    avg_val_loss = total_combined_loss / (len(val_loader) * NUM_VAL_POINTS)\n","\n","    # --- (C) Log and Save Best Model ---\n","    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.6f}, Val Loss: {avg_val_loss:.6f}\")\n","\n","    # Save only if this epoch's Val Loss is the best one seen so far\n","    if avg_val_loss < best_val_loss:\n","        best_val_loss = avg_val_loss\n","        print(f\"  -> New best validation loss! Saving model to {MODEL_PATH}\")\n","        # Save the state_dict of the single model\n","        torch.save(model.state_dict(), MODEL_PATH)\n","\n","print(\"--- Training finished. ---\")\n","print(f\"Best validation loss achieved: {best_val_loss:.6f}\")\n","print(f\"Best model saved to {MODEL_PATH}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9GzLK_7wvmuq","executionInfo":{"status":"ok","timestamp":1761997299513,"user_tz":-540,"elapsed":2270854,"user":{"displayName":"종합설계","userId":"01573912834551956230"}},"outputId":"68fcb3c4-3e45-455f-e741-6eb60e95b252"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","Starting robust training (Random SNR)...\n","Epoch [1/20], Train Loss: 0.090234, Val Loss: 0.042487\n","  -> New best validation loss! Saving model to /content/drive/MyDrive/models/face_autoencoder_BEST.pth\n","Epoch [2/20], Train Loss: 0.039017, Val Loss: 0.034887\n","  -> New best validation loss! Saving model to /content/drive/MyDrive/models/face_autoencoder_BEST.pth\n","Epoch [3/20], Train Loss: 0.030226, Val Loss: 0.028640\n","  -> New best validation loss! Saving model to /content/drive/MyDrive/models/face_autoencoder_BEST.pth\n","Epoch [4/20], Train Loss: 0.028206, Val Loss: 0.026721\n","  -> New best validation loss! Saving model to /content/drive/MyDrive/models/face_autoencoder_BEST.pth\n","Epoch [5/20], Train Loss: 0.024846, Val Loss: 0.024281\n","  -> New best validation loss! Saving model to /content/drive/MyDrive/models/face_autoencoder_BEST.pth\n","Epoch [6/20], Train Loss: 0.023079, Val Loss: 0.023559\n","  -> New best validation loss! Saving model to /content/drive/MyDrive/models/face_autoencoder_BEST.pth\n","Epoch [7/20], Train Loss: 0.022704, Val Loss: 0.022645\n","  -> New best validation loss! Saving model to /content/drive/MyDrive/models/face_autoencoder_BEST.pth\n","Epoch [8/20], Train Loss: 0.021117, Val Loss: 0.021898\n","  -> New best validation loss! Saving model to /content/drive/MyDrive/models/face_autoencoder_BEST.pth\n","Epoch [9/20], Train Loss: 0.020374, Val Loss: 0.021422\n","  -> New best validation loss! Saving model to /content/drive/MyDrive/models/face_autoencoder_BEST.pth\n","Epoch [10/20], Train Loss: 0.019531, Val Loss: 0.019958\n","  -> New best validation loss! Saving model to /content/drive/MyDrive/models/face_autoencoder_BEST.pth\n","Epoch [11/20], Train Loss: 0.019109, Val Loss: 0.019833\n","  -> New best validation loss! Saving model to /content/drive/MyDrive/models/face_autoencoder_BEST.pth\n","Epoch [12/20], Train Loss: 0.018089, Val Loss: 0.020088\n","Epoch [13/20], Train Loss: 0.017985, Val Loss: 0.019274\n","  -> New best validation loss! Saving model to /content/drive/MyDrive/models/face_autoencoder_BEST.pth\n","Epoch [14/20], Train Loss: 0.017373, Val Loss: 0.018685\n","  -> New best validation loss! Saving model to /content/drive/MyDrive/models/face_autoencoder_BEST.pth\n","Epoch [15/20], Train Loss: 0.017551, Val Loss: 0.018886\n","Epoch [16/20], Train Loss: 0.016949, Val Loss: 0.018223\n","  -> New best validation loss! Saving model to /content/drive/MyDrive/models/face_autoencoder_BEST.pth\n","Epoch [17/20], Train Loss: 0.016559, Val Loss: 0.021100\n","Epoch [18/20], Train Loss: 0.016720, Val Loss: 0.017694\n","  -> New best validation loss! Saving model to /content/drive/MyDrive/models/face_autoencoder_BEST.pth\n","Epoch [19/20], Train Loss: 0.015881, Val Loss: 0.017809\n","Epoch [20/20], Train Loss: 0.015999, Val Loss: 0.017577\n","  -> New best validation loss! Saving model to /content/drive/MyDrive/models/face_autoencoder_BEST.pth\n","--- Training finished. ---\n","Best validation loss achieved: 0.017577\n","Best model saved to /content/drive/MyDrive/models/face_autoencoder_BEST.pth\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import numpy as np\n","import os\n","import random\n","from PIL import Image\n","\n","# --- 1. Module and Dataloader Setup (Assuming these are defined earlier) ---\n","from channels import awgn_channel\n","from face_autoencoder import FaceAutoencoder\n","\n","# (Assume val_loader, device, MODEL_PATH, etc., are loaded)\n","\n","# -----------------------------------------------\n","# 2. Configuration for Visualization\n","# -----------------------------------------------\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","SAVE_DIR = \"/content/drive/MyDrive/models\"\n","MODEL_PATH = os.path.join(SAVE_DIR, \"face_autoencoder_BEST.pth\")\n","VISUALIZE_SNR_DB = 5.0\n","NUM_IMAGES_TO_SHOW = 8\n","OUTPUT_FOLDER = \"./reconstructions\" # 새로운 출력 폴더\n","os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n","\n","# -----------------------------------------------\n","# 3. Load the Best Model (Identical to previous code)\n","# -----------------------------------------------\n","print(f\"Loading best model from: {MODEL_PATH}\")\n","model = FaceAutoencoder(latent_dim=256).to(device)\n","try:\n","    model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n","    model.eval()\n","    print(\"Model loaded successfully and set to eval mode.\")\n","except FileNotFoundError:\n","    print(f\"[ERROR] Model file not found at {MODEL_PATH}. Exiting.\")\n","    exit()\n","\n","# -----------------------------------------------\n","# 4. Get a Batch of Validation Images (Identical to previous code)\n","# -----------------------------------------------\n","try:\n","    data_iter = iter(val_loader)\n","    images, _ = next(data_iter)\n","    images = images.to(device)\n","    sample_images = images[:NUM_IMAGES_TO_SHOW]\n","except NameError:\n","    print(\"[ERROR] val_loader is not defined. Please run the Dataloader setup cell first. Exiting.\")\n","    exit()\n","except StopIteration:\n","    print(\"[ERROR] val_loader is empty or finished. Try rerunning the Dataloader cell. Exiting.\")\n","    exit()\n","\n","# -----------------------------------------------\n","# 5. Process Images through the Autoencoder (Identical to previous code)\n","# -----------------------------------------------\n","with torch.no_grad():\n","    latent_vector_original = model.encode(sample_images)\n","    noisy_latent_vector = awgn_channel(latent_vector_original, snr_db=VISUALIZE_SNR_DB)\n","    reconstructed_noisy = model.decode(noisy_latent_vector)\n","    reconstructed_pristine = model.decode(latent_vector_original)\n","\n","# --- NEW: Image Saving Function using PIL ---\n","def save_image_to_pil(tensor, filename):\n","    # 텐서를 NumPy 배열로 변환\n","    img_np = tensor.cpu().numpy()\n","\n","    # 텐서 크기: (C, H, W) -> PIL 크기: (H, W, C)로 변환\n","    img_np = np.transpose(img_np, (1, 2, 0))\n","\n","    # Tanh 출력 [-1, 1] -> 픽셀 값 [0, 255]로 변환 및 uint8 타입으로 변경\n","    # (x / 2 + 0.5) * 255 = (x + 1) * 127.5\n","    img_np = (img_np * 127.5) + 127.5\n","    img_np = np.clip(img_np, 0, 255).astype(np.uint8)\n","\n","    # PIL Image 객체 생성 및 저장\n","    img_pil = Image.fromarray(img_np)\n","    img_pil.save(os.path.join(OUTPUT_FOLDER, filename))\n","\n","print(f\"\\nSaving {NUM_IMAGES_TO_SHOW} image triples to '{OUTPUT_FOLDER}/'...\")\n","\n","# --- 6. Save the Results ---\n","image_list = [sample_images, reconstructed_noisy, reconstructed_pristine]\n","prefix_list = [\"Original\", f\"Recon_Noisy_{VISUALIZE_SNR_DB}dB\", \"Recon_Pristine\"]\n","\n","for i in range(NUM_IMAGES_TO_SHOW):\n","    for j, (image_set, prefix) in enumerate(zip(image_list, prefix_list)):\n","        save_image_to_pil(image_set[i], f\"{i:02d}_{prefix}.png\")\n","\n","print(f\"Saving complete. Please check the '{OUTPUT_FOLDER}' folder for image files.\")\n","\n","# (Optional: If in Colab, you can zip the folder for easy download)\n","# !zip -r reconstructions.zip {OUTPUT_FOLDER}\n","# from google.colab import files\n","# files.download('reconstructions.zip')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AfyzQfwLOcOF","executionInfo":{"status":"ok","timestamp":1761997492505,"user_tz":-540,"elapsed":1036,"user":{"displayName":"종합설계","userId":"01573912834551956230"}},"outputId":"87e85c03-a5f5-47be-a786-777a66fe51c7"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading best model from: /content/drive/MyDrive/models/face_autoencoder_BEST.pth\n","Model loaded successfully and set to eval mode.\n","\n","Saving 8 image triples to './reconstructions/'...\n","Saving complete. Please check the './reconstructions' folder for image files.\n"]}]}]}